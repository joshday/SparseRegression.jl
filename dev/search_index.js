var documenterSearchIndex = {"docs":
[{"location":"usage/#Usage-1","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"Create a model\nFit the model","category":"page"},{"location":"usage/#SModel-1","page":"Usage","title":"SModel","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"The model type used by SparseRegression is SModel.  An SModel holds onto the sufficient information for generating a solution fo the SparseRegression objective.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"note: Note\nConstructing an SModel does not create a fitted model.  It must be learn!-ed.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"SModel","category":"page"},{"location":"usage/#SparseRegression.SModel","page":"Usage","title":"SparseRegression.SModel","text":"SModel(x, y, args...)\n\nCreate a SparseRegression model with predictor AbstractMatrix x and response AbstractVector y.  x must have methods:\n\nmul!(::Vector{Float64}, x, ::Vector{Float64})\nmul!(::Vector{Float64}, x', ::Vector{Float64})\n\nThe additional arguments can be given in any order.\n\nArguments\n\nloss::Loss = .5 * L2DistLoss()\npenalty::Penalty = L2Penalty()\nλ::Vector{Float64} = fill(size(x, 2), .1)\nw::Union{Nothing, AbstractWeights} = nothing\n\nExample\n\nx = randn(1000, 5)\ny = x * range(-1, stop=1, length=5) + randn(1000)\ns = SModel(x, y)\nlearn!(s)\ns\n\n\n\n\n\n","category":"type"},{"location":"usage/#[LearningStrategies](https://github.com/JuliaML/LearningStrategies.jl)-1","page":"Usage","title":"LearningStrategies","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"An SModel can be learned with the default learning strategy with learn!(model).  You can provide more control over the learning process by providing your own LearningStrategy.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"SparseRegression implements several Algorithm <: LearningStrategy types to do SModel fitting.  An Algorithm must be constructed with an SModel to ensure storage buffers are the correct size.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"using SparseRegression\n\n# Make some fake data\nx = randn(1000, 10)\ny = x * range(-1, stop=1, length=10) + randn(1000)\n\n# Create an SModel\ns = SModel(x, y)\n\n# All of the following are valid ways to calculate a solution\nlearn!(s)\nlearn!(s, strategy(ProxGrad(s), MaxIter(25), TimeLimit(.5)))\nlearn!(s, Sweep(s))\nlearn!(s, LinRegCholesky(s))","category":"page"},{"location":"#Introduction-1","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"SparseRegression is a Julia package which combines JuliaML primitives to implement high-performance algorithms for fitting linear models.","category":"page"},{"location":"#Objective-Function-1","page":"Introduction","title":"Objective Function","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"The objective function that SparseRegression can solve takes the form:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"frac1nsum_i=1^n w_i f(y_i x_i^Tbeta) + sum_j=1^p lambda_j J(beta_j)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"where f is a loss function, J is a penalty or regularization function, the w_i's are nonnegative observation weights and the lambda_j's are nonnegative element-wise regularization parameters.  Many models take this form:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Model f(y_i x_i^Tbeta) g(beta_j)\nLasso Regression frac12(y_i - x_i^Tbeta)^2 beta_j\nRidge Regression frac12(y_i - x_i^Tbeta)^2 beta_j^2\nSVM max(0 1 - y_i x_i^Tbeta) beta_j^2","category":"page"},{"location":"#[JuliaML](https://github.com/JuliaML)-1","page":"Introduction","title":"JuliaML","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"The three core JuliaML packages that SparseRegression brings together are:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"LossFunctions\nPenaltyFunctions\nLearningStrategies","category":"page"},{"location":"algorithms/#Algorithms-1","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"The first argument of an Algorithm's constructor is an SModel.  This is to ensure storage buffers are the correct size.","category":"page"},{"location":"algorithms/#ProxGrad-1","page":"Algorithms","title":"ProxGrad","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"ProxGrad","category":"page"},{"location":"algorithms/#SparseRegression.ProxGrad","page":"Algorithms","title":"SparseRegression.ProxGrad","text":"ProxGrad(model, step = 1.0)\n\nProximal gradient method with step size step.  Works for any loss and any penalty with a prox method.\n\nExample\n\nx, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)\ns = SModel(x, y, L2DistLoss())\nstrat = strategy(MaxIter(50), ProxGrad(s))\nlearn!(s, strat)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Fista-1","page":"Algorithms","title":"Fista","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"Fista","category":"page"},{"location":"algorithms/#SparseRegression.Fista","page":"Algorithms","title":"SparseRegression.Fista","text":"Fista(model, step = 1.0)\n\nAccelerated proximal gradient method.  Works for any loss and any penalty with a prox method.\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#AdaptiveProxGrad-1","page":"Algorithms","title":"AdaptiveProxGrad","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"AdaptiveProxGrad","category":"page"},{"location":"algorithms/#SparseRegression.AdaptiveProxGrad","page":"Algorithms","title":"SparseRegression.AdaptiveProxGrad","text":"AdaptiveProxGrad(s, divisor = 1.5, init = 1.0)\n\nProximal gradient method with adaptive step sizes.  AdaptiveProxGrad uses element-wise  learning rates.  Every time the sign of a coefficient switches, the step size for that coefficient is divided by divisor.\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#GradientDescent-1","page":"Algorithms","title":"GradientDescent","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"GradientDescent","category":"page"},{"location":"algorithms/#SparseRegression.GradientDescent","page":"Algorithms","title":"SparseRegression.GradientDescent","text":"GradientDescent(model, step = 1.0)\n\nGradient Descent.  Works for any loss and any penalty.\n\nExample\n\nx, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)\ns = SModel(x, y, L2DistLoss())\nstrat = strategy(MaxIter(50), GradientDescent(s))\nlearn!(s, strat)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Sweep-1","page":"Algorithms","title":"Sweep","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"Sweep","category":"page"},{"location":"algorithms/#SparseRegression.Sweep","page":"Algorithms","title":"SparseRegression.Sweep","text":"Sweep(model)\n\nLinear/ridge regression via sweep operator.  Works for (scaled) L2DistLoss with NoPenalty or L2Penalty.  The Sweep algorithm has a closed form solution and is complete after one iteration.  It therefore doesn't need additional learning strategies such as MaxIter, Converged, etc.\n\nExample\n\nx, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)\ns = SModel(x, y, L2DistLoss())\nlearn!(s, Sweep(s))\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#LinRegCholesky-1","page":"Algorithms","title":"LinRegCholesky","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"LinRegCholesky","category":"page"},{"location":"algorithms/#SparseRegression.LinRegCholesky","page":"Algorithms","title":"SparseRegression.LinRegCholesky","text":"LinRegCholesky(model)\n\nLinear/ridge regression via cholesky decomposition.  Works for (scaled) L2DistLoss with NoPenalty or L2Penalty.  The LinRegCholesky algorithm has a closed form solution  and is complete after one iteration.  It therefore doesn't need additional learning strategies such as MaxIter, Converged, etc.\n\nExample\n\nx, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)\ns = SModel(x, y, L2DistLoss())\nlearn!(s, Sweep(s))\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#LineSearch-1","page":"Algorithms","title":"LineSearch","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"LineSearch","category":"page"},{"location":"algorithms/#SparseRegression.LineSearch","page":"Algorithms","title":"SparseRegression.LineSearch","text":"LineSearch(algorithm)\n\nUse a line search in the update! of algorithm.  Currently, ProxGrad, Fista, and GradientDescent are supported.\n\nExample\n\nx, y, β = SparseRegression.fakedata(L2DistLoss(), 1000, 10)\ns = SModel(x, y, L2DistLoss())\nstrat = strategy(MaxIter(50), LineSearch(ProxGrad(s)))\nlearn!(s, strat)\n\n\n\n\n\n","category":"type"}]
}
